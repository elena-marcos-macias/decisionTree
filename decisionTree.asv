%--------- Add function path and set save path ----------
addpath (genpath('./utils'));
addpath ('./requirements');
savePath = './results/';


%% TRAINING DATASET

%--------- Load data ---------
fileName = char("FUS_test6.xlsx");
T_Original = readtable(['./data/' fileName]);

% -------- Remove rows with NaN --------
T_Original = rmmissing(T_Original);  % Remove rows that contain at least one NaN

% -------- Select data to use (columns with string or numerical criteria) ---------------
target_columns = [1,22];
ignore_columns = [2,3];
T_Data = selectColumns (T_Original, target_columns, ignore_columns);
T_ResultsVariable = T_Original.Death;

%% Identify minority and majority classes
[classNames, ~, idxClass] = unique(T_ResultsVariable);
counts = accumarray(idxClass, 1);

[~, majorityIdx] = max(counts);
[~, minorityIdx] = min(counts);

majorityClass = classNames(majorityIdx);
minorityClass = classNames(minorityIdx);

fprintf('Majority class: %s (%d samples)\n', majorityClass{1}, counts(majorityIdx));
fprintf('Minority class: %s (%d samples)\n', minorityClass{1}, counts(minorityIdx));


%% CROSS-VALIDATION TREE

% ------------------ 1) TREE ------------------ 
CVMdl = fitctree( ...
    T_Data, T_ResultsVariable, ...
    'KFold',           5, ...
    'CategoricalPredictors', {'Genotype'}, ...
    'MinParentSize',   3);

% ------------------ 2) MODEL'S RESULTS ------------------------
Label = kfoldPredict(CVMdl);

% --- Misclassification rate ---
missClassRate = kfoldLoss(CVMdl);
fprintf('5-fold CV loss: %.3f\n', missClassRate);

% --- Class-wise misclassification ---
[missMajority, missMinority] = classwiseMisclassification(T_ResultsVariable, Label, majorityClass, minorityClass);
fprintf('Standard CV - Majority class misclassification: %.3f\n', missMajority);
fprintf('Standard CV - Minority class misclassification: %.3f\n', missMinority);


%% CROSS-VALIDATION TREE WITH WEIGHTS

% --- Compute class frequencies ---
[classNames, ~, idxClass] = unique(T_ResultsVariable);      % classNames is a cell array of labels
nClasses = numel(classNames);                               % number of classes
counts   = accumarray(idxClass, 1);                         % count per class
nTotal   = numel(T_ResultsVariable);                        % total number of observations

% --- Compute per-class weights: w_c ∝ 1/counts(c) ---
% Normalize so that sum of all sample weights = nTotal
invFreq   = 1 ./ counts;                                    % inverse frequency per class
normFactor = nTotal / sum(counts .* invFreq);
classWeights = normFactor .* invFreq;                       % vector of length nClasses

% --- Build a weight vector W matching observations ---
W = classWeights(idxClass);                 % maps each sample to its class weight

% ------------------ 2) WEIGHTED TREE ------------------ 
WeightCVMdl = fitctree( ...
    T_Data, T_ResultsVariable, ...
    'KFold',           5, ...
    'Weights',         W, ...
    'CategoricalPredictors', {'Genotype'}, ...
    'MinParentSize',   3);

% ------------------ 3) MODEL'S RESULTS ------------------------
wtLabel  = kfoldPredict(WeightCVMdl);

% --- Misclassification rate ---
missClassRateWeight  = kfoldLoss(WeightCVMdl); 
fprintf('Weighted   5-fold CV loss: %.3f\n', missClassRateWeight);

% --- Class-wise misclassification ---
[missMajorityW, missMinorityW] = classwiseMisclassification(T_ResultsVariable, wtLabel, majorityClass, minorityClass);
fprintf('Weighted CV - Majority class misclassification: %.3f\n', missMajorityW);
fprintf('Weighted CV - Minority class misclassification: %.3f\n', missMinorityW);


%% CROSS-VALIDATION TREE WITH OVERSAMPLING

% --- Create 5-fold partition ---
cv = cvpartition(T_ResultsVariable, 'KFold', 5);

% --- Initialize prediction vector ---
OSLabels = strings(size(T_ResultsVariable));

for i = 1:cv.NumTestSets
    % Indices for training and testing
    trainIdx = training(cv, i);
    testIdx  = test(cv, i);

    % Split data
    XTrain = T_Data(trainIdx,:);
    YTrain = T_ResultsVariable(trainIdx);
    XTest = T_Data(testIdx,:);
    YTest = T_ResultsVariable(testIdx);

    % ------------------ OVERSAMPLING -------------------
    % Find class counts
    classNames = unique(YTrain);
    counts = groupcounts(YTrain);
    maxCount = max(counts);

    XBalanced = XTrain;
    YBalanced = YTrain;

    for j = 1:numel(classNames)
        cls = classNames(j);
        idx = find(strcmp(YTrain, cls));

        % Compute how many more samples needed
        nToAdd = maxCount - numel(idx);

        if nToAdd > 0
            % Randomly sample with replacement
            sampledIdx = datasample(idx, nToAdd, 'Replace', true);
            XBalanced = [XBalanced; XTrain(sampledIdx,:)];
            YBalanced = [YBalanced; YTrain(sampledIdx)];
        end
    end

    % ------------------ Train model --------------------
    OSCVMdl = fitctree(...
        XBalanced, YBalanced, ...
        'CategoricalPredictors', {'Genotype'}, ...
        'MinParentSize', 3);

    % ------------------ Predict ------------------------
    YPred = predict(OSCVMdl, XTest);

    % Save predictions
    OSLabels(testIdx) = YPred;
end

% ------------------ MODEL'S RESULTS ------------------------
OSLabels = cellstr(OSLabels);

% --- Misclassification rate ---
missClassRateOS = sum(~strcmp(OSLabels, T_ResultsVariable)) / numel(T_ResultsVariable);
fprintf('Oversampled CV Misclassification Rate: %.3f\n', missClassRateOS);

% --- Class-wise misclassification ---
[missMajorityOS, missMinorityOS] = classwiseMisclassification(T_ResultsVariable, OSLabels, majorityClass, minorityClass);
fprintf('Oversampled CV - Majority class misclassification: %.3f\n', missMajorityOS);
fprintf('Oversampled CV - Minority class misclassification: %.3f\n', missMinorityOS);


%% TRAIN THE MODEL

Mdl = fitctree(T_Data, T_ResultsVariable, 'CategoricalPredictors', {'Genotype'}, 'MinParentSize',3);

% VIEW THE TREE
view(Mdl,'Mode','graph');
savefig(fullfile(savePath, 'DecisionTree.fig'));

% PREDICTORS' IMPORTANCE
imp = predictorImportance(Mdl);
predictorNames = Mdl.PredictorNames;

figure;
bar(imp);
title('Predictor Importance Estimates');
ylabel('Importance');
xlabel('Predictors');

ax = gca;
ax.XTick = 1:numel(predictorNames);
ax.XTickLabel = predictorNames;
ax.XTickLabelRotation = 45;
ax.TickLabelInterpreter = 'none';

savefig(fullfile(savePath, 'PredictorImportance.fig'));

%% TEST DATASET

DeathFUS = predict(Mdl,T_Data);

% ------------------ Evaluate model ------------------------
nTotal = numel(T_ResultsVariable);
nIncorrect = sum(~strcmp(DeathFUS, T_ResultsVariable));

testError = nIncorrect / nTotal;
fprintf('Test Error (on training data): %.3f\n', testError);

% --- Class-wise misclassification ---
[missMajorityFinal, missMinorityFinal] = classwiseMisclassification(T_ResultsVariable, DeathFUS, majorityClass, minorityClass);
fprintf('Final Model - Majority class misclassification: %.3f\n', missMajorityFinal);
fprintf('Final Model - Minority class misclassification: %.3f\n', missMinorityFinal);


%% CONFUSION CHART

trueLabels = string(T_ResultsVariable);
Label = string(Label);
wtLabel = string(wtLabel);
OSLabels = string(OSLabels);
trainingDatasetPredictions = string(DeathFUS);

figure('Units', 'normalized', 'Position', [0.1 0.3 0.8 0.4]);
set(gca,'DataAspectRatio',[6 .5 0.2]);
sgtitle ('Confusion Matrices per model');

subplot(1,4,1);
confusionchart(trueLabels, Label);
title('Standard 5-fold cross-validation');

subplot(1,4,2);
confusionchart(trueLabels, wtLabel);
title('Weighted 5-f cv');

subplot(1,4,3);
confusionchart(trueLabels, OSLabels);
title('Oversampled 5-f cv');

subplot(1,4,4);
confusionchart(trueLabels, trainingDatasetPredictions);
title('Final Model (with Training Data)');

savefig(fullfile(savePath, 'ConfusionCharts.fig'));


 %% CREATE AN EXCEL FILE 